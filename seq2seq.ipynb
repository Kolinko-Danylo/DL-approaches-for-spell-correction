{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_with_attention",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6LLbz8mJMmY",
        "colab_type": "code",
        "outputId": "44ee8882-762e-4706-88ae-2c93341110e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "!pip3 install bpemb\n",
        "!pip3 install nlpaug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.17.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.83)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.28.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.9.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (1.10.18)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->bpemb) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.18 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (1.13.18)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim->bpemb) (0.2.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->smart-open>=1.2.1->gensim->bpemb) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.18->boto3->smart-open>=1.2.1->gensim->bpemb) (2.6.1)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.6/dist-packages (0.0.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFdtxnGsoP4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "import socket\n",
        "hostname = socket.gethostname()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_uD6M3MsRAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bpemb import BPEmb\n",
        "import textwrap\n",
        "import nlpaug.augmenter.char as nac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGa5R1cxr5gh",
        "colab_type": "code",
        "outputId": "d92b86f2-0fa0-4653-da8c-08fbe383b70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\", dim=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSg_YFsyr5mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('big.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8mZA6pdfaM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URvJKox6YznH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_encodes(arr, seq_length, use_aug=False):\n",
        "  if use_aug:\n",
        "    aug_rr = nac.KeyboardAug(aug_char_min=0, aug_char_max=None, aug_char_p=0.4, aug_word_p=0.4, aug_word_min=0, aug_word_max=arr.size//3, special_char=False)\n",
        "    \n",
        "    augmented_data = list(map(lambda x: aug_rr.augment(x), arr.ravel()))\n",
        "    arr = np.array(augmented_data).reshape(arr.shape)\n",
        "  flat_arr = arr.ravel()\n",
        "\n",
        "  def padded_encode(x):\n",
        "    k = np.zeros((seq_length,))\n",
        "    enc = np.array(bpemb_en.encode_ids(x))\n",
        "    k[:enc.size] = enc\n",
        "    return enc.size, k\n",
        "\n",
        "  res_arr = np.empty((*flat_arr.shape, seq_length), dtype=\"int32\")\n",
        "  len_vec = np.empty( (arr.shape[0]))\n",
        "  for i in range(flat_arr.size):\n",
        "\n",
        "    res = padded_encode(flat_arr[i])  \n",
        "    res_arr[i] = res[1]\n",
        "    len_vec[i] = res[0]\n",
        "\n",
        "  if not use_aug:\n",
        "    res_arr = np.insert(res_arr, 0, 1, 1)\n",
        "  len_vec, perm_idx = torch.from_numpy(len_vec).sort(0, descending=True)\n",
        "  res_arr = res_arr[perm_idx]\n",
        "\n",
        "  leng, res = len_vec, one_hot_encode(res_arr, 10000)\n",
        "\n",
        "  leng += (1 if not use_aug else 0)\n",
        "  leng, res = leng, torch.from_numpy(res)\n",
        "\n",
        "  return leng, res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xAru18_FBm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size]    \n",
        "\n",
        "  \n",
        "\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1]):\n",
        "        # The features\n",
        "        base = arr[:, n:n+1]\n",
        "        # y = np.vectorize(get_int)(base)\n",
        "\n",
        "        x = base.copy()\n",
        "        # y = one_hot_encode(y, len(words))\n",
        "        lengths_x, x = get_encodes(x, seq_length, use_aug=True)\n",
        "        lengths_y, y = get_encodes(base, seq_length)\n",
        "        \n",
        "        yield lengths_x, x, lengths_y, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOGrQItGFBlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "arr = np.array(textwrap.wrap(text=text, width=5))\n",
        "\n",
        "batches = get_batches(arr, 2, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOHqalBXFBja",
        "colab_type": "code",
        "outputId": "2363798e-848d-485d-f9c6-89c123acabd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lengths_x, x, lengths_y, y = next(batches)\n",
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 10000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrc0kTuEZ8VW",
        "colab_type": "text"
      },
      "source": [
        "### Hide\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cze_2BXWFBfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True, batch_first=True)\n",
        "        \n",
        "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
        "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths, batch_first=True)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True) # unpack (back to padded)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkX9fH5lYTV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(0)\n",
        "        this_batch_size = encoder_outputs.size(1)\n",
        "\n",
        "        # Create variable to store attention energies\n",
        "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
        "\n",
        "\n",
        "        attn_energies = attn_energies.cuda()\n",
        "\n",
        "        # For each batch of encoder outputs\n",
        "        for b in range(this_batch_size):\n",
        "            # Calculate energy for each encoder output\n",
        "            for i in range(max_len):\n",
        "                print(hidden[:, b].shape, encoder_outputs[i, b].shape)\n",
        "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
        "\n",
        "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
        "        return F.softmax(attn_energies).unsqueeze(1)\n",
        "    \n",
        "    def score(self, hidden, encoder_output):\n",
        "        \n",
        "        if self.method == 'dot':\n",
        "            energy = hidden.dot(encoder_output)\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'general':\n",
        "            energy = self.attn(encoder_output)\n",
        "            energy = hidden.dot(energy)\n",
        "            return energy\n",
        "        \n",
        "        elif self.method == 'concat':\n",
        "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
        "            energy = self.v.dot(energy)\n",
        "            return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkNv2CgzYWGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, input_size, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Choose attention model\n",
        "        if attn_model != 'none':\n",
        "            self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
        "\n",
        "        rnn_output, hidden = self.gru(input_seq, last_hidden)\n",
        "\n",
        "        # Calculate attention from current RNN state and all encoder outputs;\n",
        "        # apply to encoder outputs to get weighted average\n",
        "        # print(rnn_output.shape, encoder_outputs.shape)\n",
        "        # attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
        "\n",
        "        # Attentional vector using the RNN hidden state and context vector\n",
        "        # concatenated together (Luong eq. 5)\n",
        "        # rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        # context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        # concat_input = torch.cat((rnn_output, context), 1)\n",
        "        # concat_output = F.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Finally predict next token (Luong eq. 6, without softmax)\n",
        "        output = self.out(rnn_output)\n",
        "\n",
        "        # Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden#, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voeWkOiS5w_h",
        "colab_type": "code",
        "outputId": "35f6d460-295b-4461-afad-fe2f1457677f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "seq_length = 1000\n",
        "small_batch_size = 3\n",
        "data = np.array(textwrap.wrap(text=text, width=seq_length))\n",
        "\n",
        "batches = get_batches(data, small_batch_size, seq_length=seq_length)\n",
        "input_lengths, input_batches, target_lengths, target_batches = next(batches)\n",
        "for i in [input_lengths, input_batches, target_lengths, target_batches]:\n",
        "  print(i.shape)\n",
        "  i = i.cuda()\n",
        "\n",
        "print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
        "print('target_batches', target_batches.size()) # (max_len x batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([3, 1000, 10000])\n",
            "torch.Size([3])\n",
            "torch.Size([3, 1001, 10000])\n",
            "input_batches torch.Size([3, 1000, 10000])\n",
            "target_batches torch.Size([3, 1001, 10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5QIulIkr5kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_hidden_size = 1000\n",
        "small_n_layers = 2\n",
        "\n",
        "decoder_test = LuongAttnDecoderRNN('general', CLASSES, small_hidden_size, CLASSES, small_n_layers)\n",
        "\n",
        "encoder_test = EncoderRNN(10000, small_hidden_size, small_n_layers)\n",
        "\n",
        "encoder_test = encoder_test.cuda()\n",
        "decoder_test = decoder_test.cuda()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QusrqArg5f6z",
        "colab_type": "code",
        "outputId": "68092b54-bef0-4512-b590-f6ff99d05058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "encoder_outputs, encoder_hidden = encoder_test(input_batches.cuda(), input_lengths.cuda(), None)\n",
        "\n",
        "print('encoder_outputs', encoder_outputs.size(), encoder_hidden.size()) # max_len x batch_size x hidden_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder_outputs torch.Size([3, 288, 1000]) torch.Size([4, 3, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIMr3WJ0olM6",
        "colab_type": "code",
        "outputId": "81cdeb36-473f-41aa-cd8a-064089130afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "target_batches[:, 1:].argmax(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1788,  146, 1988,  ...,    0,    0,    0],\n",
              "        [  42, 9945,  555,  ...,    0,    0,    0],\n",
              "        [   7, 1361,   51,  ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knp1t2vegh54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogzg3ZyTBccy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = decoder_test(target_batches[:, :-1].cuda(), encoder_hidden[:decoder_test.n_layers], encoder_outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYoDYTjAgTIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NiQzrIJb9b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7o8Zyc7dOoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tar = target_batches[:, 1:].argmax(2)\n",
        "tar = tar.view(tar.size(0)*tar.size(1))\n",
        "cur = l[0].view(l[0].size(0)*l[0].size(1), -1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L_PzMmFmNyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tTVUO1pcDga",
        "colab_type": "code",
        "outputId": "bc778dae-244c-447a-c778-9b2b7385310d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "criterion(l[0], target_batches[:, 1:].argmax(2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-bbddfc3ad4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 1848\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected target size (3, 10000), got torch.Size([3, 1000])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkA-zvcdo5wU",
        "colab_type": "code",
        "outputId": "97777569-2467-4c9d-c5cf-783ae53eb28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "CLASSES = 10000\n",
        "hidden_size = 600\n",
        "decoder = LuongAttnDecoderRNN('general', CLASSES, hidden_size, CLASSES, n_layers=2)\n",
        "\n",
        "encoder = EncoderRNN(CLASSES, hidden_size, n_layers=2)\n",
        "\n",
        "sl = 80\n",
        "data = np.array(textwrap.wrap(text=text, width=sl))\n",
        "\n",
        "train(encoder, decoder, data, epochs=1, batch_size=100, seq_length=sl, hidden_size=hidden_size, lr=0.001, clip=5, val_frac=0.1, print_every=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/1... Step: 20... Loss: 2.0075... Val Loss: 1.9093\n",
            "Epoch: 1/1... Step: 40... Loss: 1.7845... Val Loss: 1.7611\n",
            "Epoch: 1/1... Step: 60... Loss: 1.7087... Val Loss: 1.7186\n",
            "Epoch: 1/1... Step: 80... Loss: 1.7535... Val Loss: 1.7425\n",
            "Epoch: 1/1... Step: 100... Loss: 1.7237... Val Loss: 1.7370\n",
            "Epoch: 1/1... Step: 120... Loss: 1.6854... Val Loss: 1.6971\n",
            "Epoch: 1/1... Step: 140... Loss: 1.7274... Val Loss: 1.7642\n",
            "Epoch: 1/1... Step: 160... Loss: 1.7060... Val Loss: 1.6879\n",
            "Epoch: 1/1... Step: 180... Loss: 1.6150... Val Loss: 1.6796\n",
            "Epoch: 1/1... Step: 200... Loss: 1.7107... Val Loss: 1.7205\n",
            "Epoch: 1/1... Step: 220... Loss: 1.7046... Val Loss: 1.6846\n",
            "Epoch: 1/1... Step: 240... Loss: 1.6657... Val Loss: 1.6753\n",
            "Epoch: 1/1... Step: 260... Loss: 1.6639... Val Loss: 1.6716\n",
            "Epoch: 1/1... Step: 280... Loss: 1.6692... Val Loss: 1.7321\n",
            "Epoch: 1/1... Step: 300... Loss: 1.6813... Val Loss: 1.6743\n",
            "Epoch: 1/1... Step: 320... Loss: 1.6739... Val Loss: 1.6897\n",
            "Epoch: 1/1... Step: 340... Loss: 1.6542... Val Loss: 1.7159\n",
            "Epoch: 1/1... Step: 360... Loss: 1.7046... Val Loss: 1.7347\n",
            "Epoch: 1/1... Step: 380... Loss: 1.6683... Val Loss: 1.7694\n",
            "Epoch: 1/1... Step: 400... Loss: 1.7055... Val Loss: 1.7352\n",
            "Epoch: 1/1... Step: 420... Loss: 1.5881... Val Loss: 1.6996\n",
            "Epoch: 1/1... Step: 440... Loss: 1.6825... Val Loss: 1.7080\n",
            "Epoch: 1/1... Step: 460... Loss: 1.6279... Val Loss: 1.7139\n",
            "Epoch: 1/1... Step: 480... Loss: 1.6372... Val Loss: 1.6741\n",
            "Epoch: 1/1... Step: 500... Loss: 1.6342... Val Loss: 1.6871\n",
            "Epoch: 1/1... Step: 520... Loss: 1.6204... Val Loss: 1.6448\n",
            "Epoch: 1/1... Step: 540... Loss: 1.5704... Val Loss: 1.6560\n",
            "Epoch: 1/1... Step: 560... Loss: 1.6162... Val Loss: 1.6499\n",
            "Epoch: 1/1... Step: 580... Loss: 1.6106... Val Loss: 1.6226\n",
            "Epoch: 1/1... Step: 600... Loss: 1.5803... Val Loss: 1.6917\n",
            "Epoch: 1/1... Step: 620... Loss: 1.5476... Val Loss: 1.5949\n",
            "Epoch: 1/1... Step: 640... Loss: 1.5036... Val Loss: 1.5923\n",
            "Epoch: 1/1... Step: 660... Loss: 1.5226... Val Loss: 1.6241\n",
            "Epoch: 1/1... Step: 680... Loss: 1.4834... Val Loss: 1.5815\n",
            "Epoch: 1/1... Step: 700... Loss: 1.4802... Val Loss: 1.5963\n",
            "Epoch: 1/1... Step: 720... Loss: 1.5048... Val Loss: 1.5430\n",
            "Epoch: 1/1... Step: 740... Loss: 1.4233... Val Loss: 1.5617\n",
            "Epoch: 1/1... Step: 760... Loss: 1.4306... Val Loss: 1.5605\n",
            "Epoch: 1/1... Step: 780... Loss: 1.4433... Val Loss: 1.5769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAklApHnxkCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh1hGNpUl7Bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(encoder, decoder, data, epochs=10, batch_size=30, seq_length=500, hidden_size=1000, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    opt1 = torch.optim.Adam(encoder.parameters(), lr=lr)\n",
        "    opt2 = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    decoder.cuda()\n",
        "    encoder.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = None\n",
        "\n",
        "        for lenx, x, leny, y in get_batches(data, batch_size=batch_size, seq_length=seq_length):\n",
        "            counter += 1\n",
        "                        # One-hot encode our data and make them Torch tensors\n",
        "            lenx, leny = lenx.cuda(), leny.cuda()\n",
        "            inputs, targets = x.cuda(), y.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = None\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            encoder.zero_grad()\n",
        "            decoder.zero_grad()\n",
        "            \n",
        "\n",
        "            \n",
        "            # get the output from the model\n",
        "            encoder_outputs, encoder_hidden = encoder(inputs, lenx, h)\n",
        "            h_dec = encoder_outputs\n",
        "            out, h_dec = decoder(targets[:, :-1], encoder_hidden[:decoder.n_layers], h_dec)\n",
        "            tar = targets[:, 1:].argmax(2)\n",
        "            tar = tar.view(tar.size(0)*tar.size(1))\n",
        "            cur = out.view(out.size(0)*out.size(1), -1)\n",
        "            loss = criterion(cur, tar)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            # loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "            nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "            opt2.step()\n",
        "            opt1.step()\n",
        "            \n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                    # Get validation loss\n",
        "                val_h = None\n",
        "                val_losses = []\n",
        "                encoder.eval()\n",
        "                decoder.eval()\n",
        "\n",
        "                for lenx, x, leny, y in get_batches(val_data, batch_size=batch_size, seq_length=seq_length):\n",
        "                        # One-hot encode our data and make them Torch tensors\n",
        "                        \n",
        "                        # Creating new variables for the hidden state, otherwise\n",
        "                        # we'd backprop through the entire training history\n",
        "                    val_h = None\n",
        "                        \n",
        "                    inputs, targets = x, y\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    lenx, leny = lenx.cuda(), leny.cuda()\n",
        "                        \n",
        "\n",
        "                    encoder_outputs, encoder_hidden = encoder(inputs, lenx, val_h)\n",
        "                    h_dec = encoder_outputs\n",
        "                    out, h_dec = decoder(targets[:, :-1], encoder_hidden[:decoder.n_layers], h_dec)\n",
        "                    tar = targets[:, 1:].argmax(2)\n",
        "                    tar = tar.view(tar.size(0)*tar.size(1))\n",
        "                    cur = out.view(out.size(0)*out.size(1), -1)\n",
        "                    val_loss = criterion(cur, tar)\n",
        "              \n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "                    \n",
        "                encoder.train() # reset to train mode after iterationg through validation data\n",
        "                decoder.train()\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSLR1-0WxUnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, word, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        x = np.array([word])\n",
        "        x = get_encodes(x).reshape(1, 1, -1)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        print(inputs.shape)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        return p, h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX2LxHzkxUyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(encoder, decoder, sentence='The', top_k=None):\n",
        "    lst_res = []\n",
        "    encoder.eval()\n",
        "    decoder.eval() # eval mode\n",
        "    \n",
        "        \n",
        "    h = None\n",
        "    length, inputs = get_encodes(np.array(sentence[:100]), seq_length=80)\n",
        "    \n",
        "    inputs, length = inputs.cuda(), length.cuda()\n",
        "    res, h = encoder(inputs, length, h)\n",
        "    while True:\n",
        "      h_dec = res\n",
        "      zero_s = np.zeros((100, 1, 10000))\n",
        "      zero_s[:, :, 0] = 1\n",
        "      print( h[:decoder.n_layers].shape, h_dec.shape)\n",
        "      out, h_dec = decoder(zero_s, h[:decoder.n_layers], h_dec)\n",
        "\n",
        "      tar = inputs.argmax(1)\n",
        "      tar = tar.view(tar.size(0)*tar.size(1))\n",
        "      cur = out.view(out.size(0)*out.size(1), -1)\n",
        "      val_loss = criterion(cur, tar)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ktcGADTxUw_",
        "colab_type": "code",
        "outputId": "8d517978-515a-423b-e4ba-58387ac343f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "sample(encoder, decoder, data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 100, 600]) torch.Size([100, 28, 600])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-9ddc585829b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-c3ce516a23cb>\u001b[0m in \u001b[0;36msample\u001b[0;34m(encoder, decoder, sentence, top_k)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mzero_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-667a882b6c4e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Calculate attention from current RNN state and all encoder outputs;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;31m# type: (Tensor, Optional[Tensor]) -> Tuple[Tensor, Tensor]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwOR8EgFxUuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M-MFc12xUsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsV2tMfUxUip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}