{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "scRNN-regression.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "msiKrUT__x77",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "outputId": "6762a83e-922c-436f-b98d-a26f7fa46008",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "!pip install fasttext spacy nlpaug"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sFpKQqUH6WDF",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "outputId": "82aad75b-8465-41b1-8948-69905a006daf",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "!pip3 install fasttext spacy nlpaug\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import nlpaug.augmenter.char as nac\n",
    "import fasttext\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim.models.wrappers import FastText"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xvr0am8n6pt0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open('big.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_nlp.max_length = 2*len(text)\n",
    "x = spacy_nlp(text, disable=['parser', 'tagger', 'ner'])\n",
    "splitted = [token.text for token in x if not (token.text.isspace() or (token.text[0].isdigit() and token.text[-1].isdigit()))]\n",
    "model = fasttext.train_unsupervised('big.txt', dim=128)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VDmaTJkQUx9p",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eroy3RA7LxvT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "words = tuple(set(splitted))\n",
    "int2word = dict(enumerate(words))\n",
    "word2int = {ch: ii for ii, ch in int2word.items()}\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R8adKeSAUyQA",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zt6kr5yxUyjL",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def one_hot_embedding(word, n_labels):\n",
    "  return np.sum(one_hot_encode(word[1:-1], n_labels), axis=0)\n",
    "\n",
    "def get_encodes(arr, use_aug=False):\n",
    "  if use_aug:\n",
    "    aug_rr = nac.KeyboardAug(aug_char_min=0, aug_char_max=None, aug_char_p=0.4, aug_word_p=0.4, aug_word_min=0, aug_word_max=arr.size//3, special_char=False)\n",
    "    augmented_data = aug_rr.augment(\" \".join(arr.ravel().tolist())).split()\n",
    "    arr = np.array(augmented_data).reshape(arr.shape)\n",
    "  \n",
    "  flat_arr = arr.ravel()\n",
    "  splitted_encoded = np.array(list(map(lambda x: np.array([char2int[ch] for ch in x]), flat_arr)))\n",
    "\n",
    "  first_char = list(map(lambda x: x[0], splitted_encoded))\n",
    "  last_char = list(map(lambda x: x[-1], splitted_encoded))\n",
    "  middle = list(map(lambda x: x, splitted_encoded))\n",
    "\n",
    "  first_char_encoded = one_hot_encode(np.array(first_char),  len(chars))\n",
    "  last_char_encoded = one_hot_encode(np.array(last_char),  len(chars))\n",
    "\n",
    "  middle_encoded = np.vstack(list(map(lambda x: one_hot_embedding(x, len(chars)), middle)))\n",
    "  encoded_seq = np.hstack([first_char_encoded, middle_encoded, last_char_encoded]).reshape((*arr.shape, 3*len(chars)))\n",
    "  return encoded_seq\n",
    "\n",
    "\n",
    "def get_int(x):\n",
    "  return word2int[x]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MihWbD6aiAGC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]    \n",
    "\n",
    "  \n",
    "\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        base = arr[:, n:n+seq_length]\n",
    "        # y = np.vectorize(get_int)(base)\n",
    "\n",
    "        x = base.copy()\n",
    "        # y = one_hot_encode(y, len(words))\n",
    "        x = get_encodes(x, use_aug=True)\n",
    "        y = np.empty((*base.shape, 128))\n",
    "\n",
    "        for i in range(base.shape[0]):\n",
    "          for j in range(base.shape[1]):\n",
    "            y[i][j] = model.get_word_vector(base[i][j])\n",
    "        \n",
    "        yield x, y, base"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PHFkALe1iAY-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "batches = get_batches(np.array(splitted), 30, 50)\n",
    "x, y, base = next(batches)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Fii_ZhoniAB5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=650, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.rnn = nn.LSTM(3*len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # self.fc = nn.Linear(n_hidden, len(words))\n",
    "        self.fc = nn.Linear(n_hidden, 128)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        r_output, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = F.relu(self.fc(out))\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qEa0xoLO4pGJ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y, base in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = None\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            # loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss = criterion(output.double(), targets.view(seq_length*batch_size, -1))\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                val_acc = []\n",
    "                net.eval()\n",
    "                for x, y, base in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = None\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    # val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                    val_loss = criterion(output.double(), targets.view(seq_length*batch_size, -1))\n",
    "                    validation_loss.append(val_loss.item())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    ind = min(len(validation_loss), 10)\n",
    "                    current_accuracy = 0\n",
    "\n",
    "                    acc_output = output.cpu().detach().numpy().reshape((-1, 50, 128))\n",
    "                    for i in range(100):\n",
    "                      for j in range(50):\n",
    "                        if base[i][j] in [x[0] for x in helper.most_similar(positive=[acc_output[i][j]])]:\n",
    "                          current_accuracy +=1\n",
    "                    \n",
    "                    val_acc.append(current_accuracy/5000)\n",
    "                    validation_accuracy.append(current_accuracy/5000)\n",
    "                    \n",
    "\n",
    "                    if min(validation_loss[-ind:]) != min(validation_loss): break\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"Val Accuracy: {:.4f}\".format(np.mean(val_acc)))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DwUnGTGkHEnP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model.save_model(\"model.bin\")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w15a7AqYJU95",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "helper = FastText.load_fasttext_format('model.bin')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zKsqjEPh6VrG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# define and print the net\n",
    "n_hidden=650\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "batch_size = 100\n",
    "seq_length = 50\n",
    "n_epochs = 20 \n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "validation_accuracy = []\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "outputId": "dcc8f5ca-70f5-4645-c798-86fbfaef2438",
    "id": "78K3N2UZ5f-w",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    }
   },
   "source": [
    "train(net, np.array(splitted), epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 0.0255... Val Loss: 0.0246 Val Accuracy: 0.0002\n",
      "Epoch: 1/20... Step: 20... Loss: 0.0247... Val Loss: 0.0243 Val Accuracy: 0.0000\n",
      "Epoch: 1/20... Step: 30... Loss: 0.0239... Val Loss: 0.0240 Val Accuracy: 0.0002\n",
      "Epoch: 1/20... Step: 40... Loss: 0.0231... Val Loss: 0.0237 Val Accuracy: 0.0002\n",
      "Epoch: 1/20... Step: 50... Loss: 0.0232... Val Loss: 0.0231 Val Accuracy: 0.0003\n",
      "Epoch: 1/20... Step: 60... Loss: 0.0222... Val Loss: 0.0227 Val Accuracy: 0.0024\n",
      "Epoch: 1/20... Step: 70... Loss: 0.0223... Val Loss: 0.0225 Val Accuracy: 0.0370\n",
      "Epoch: 1/20... Step: 80... Loss: 0.0218... Val Loss: 0.0222 Val Accuracy: 0.0402\n",
      "Epoch: 1/20... Step: 90... Loss: 0.0220... Val Loss: 0.0220 Val Accuracy: 0.0408\n",
      "Epoch: 1/20... Step: 100... Loss: 0.0214... Val Loss: 0.0218 Val Accuracy: 0.0447\n",
      "Epoch: 1/20... Step: 110... Loss: 0.0210... Val Loss: 0.0216 Val Accuracy: 0.0518\n",
      "Epoch: 1/20... Step: 120... Loss: 0.0209... Val Loss: 0.0214 Val Accuracy: 0.0579\n",
      "Epoch: 1/20... Step: 130... Loss: 0.0207... Val Loss: 0.0212 Val Accuracy: 0.0654\n",
      "Epoch: 1/20... Step: 140... Loss: 0.0205... Val Loss: 0.0209 Val Accuracy: 0.0967\n",
      "Epoch: 1/20... Step: 150... Loss: 0.0206... Val Loss: 0.0208 Val Accuracy: 0.1078\n",
      "Epoch: 1/20... Step: 160... Loss: 0.0203... Val Loss: 0.0206 Val Accuracy: 0.1218\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aeee91caea91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-79ee5066853a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                       \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0macc_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                           \u001b[0mcurrent_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \"\"\"\n\u001b[0;32m-> 1433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/deprecated/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9XSdBzThU2SA",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "outputId": "9b8baa07-422a-4ebf-b16a-bb48ce30a7c0"
   },
   "source": [
    "import seaborn as sns\n",
    "plott = sns.lineplot(list(range(len(validation_accuracy))), validation_accuracy, color=\"blue\")\n",
    "plott.legend(['Validation Accuracy per iteration'])"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-30cdefa666f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplott\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplott\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Validation Accuracy per iteration'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_accuracy' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JoTNfborX8de",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model_name = 'scrnn_rnn_regression_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "  \n",
    "    torch.save(checkpoint, f)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RY8vDLmykCQe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def predict(net, word, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        x = np.array([word])\n",
    "        x = get_encodes(x).reshape(1, 1, -1)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        print(inputs.shape)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        return p, h"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4KhAnyl9pmHg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def sample(net, sentence='The', top_k=None):\n",
    "    lst_res = []\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    z = spacy_nlp(sentence, disable=['parser', 'tagger', 'ner'])\n",
    "    spl = [token.text for token in z if not (token.text.isspace() or (token.text[0].isdigit() and token.text[-1].isdigit()))]\n",
    "    h = net.init_hidden(1)\n",
    "    for word in spl:\n",
    "        pred, h = predict(net, word, h, top_k=top_k)\n",
    "        lst_res.append(int2word[np.argmax(pred).item()])\n",
    "\n",
    "    \n",
    "    return ' '.join(lst_res)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yjljFKzIl_PS",
    "colab_type": "code",
    "outputId": "326cde21-c099-454a-e96a-4bf7700aef12",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "sample(net, \"Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy,it deosn't mttaer in waht oredr the ltteers in a wrodare, the olny iprmoetnt tihng is taht the frist and lsatltteer be at the rghit pclae. The rset can be a toatlmses and you can sitll raed it wouthit porbelm. Tihsis bcuseae the huamn mnid deos not raed ervey lteterby istlef, but the wrod as a wlohe.\")"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n",
      "torch.Size([1, 1, 279])\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'according to a merchant at Cambridge University , it resort matter in what order the letters in a welfare , the only important thing is that the first and irregular be at the right place . The rest can be a radical and you can still read it without problem . This because the human mind does not read every liberty itself , but the word as a whole .'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 217
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xqAO35PHwnA5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "model_name = 'scRNN_5_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "  \n",
    "    torch.save(checkpoint, f)"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}